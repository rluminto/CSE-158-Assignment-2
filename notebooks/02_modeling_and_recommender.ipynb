{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b285da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from data_utils import create_binary_target, filter_sparse_users_recipes\n",
    "from features import (\n",
    "    compute_user_features, compute_recipe_features, \n",
    "    create_modeling_dataset, get_feature_columns,\n",
    "    split_temporal_per_user, prepare_features_for_training\n",
    ")\n",
    "from models import (\n",
    "    GlobalAverageBaseline, RecipeAverageBaseline, UserAverageBaseline,\n",
    "    LogisticRegressionModel, tune_hyperparameters, RecipeRecommender\n",
    ")\n",
    "from eval_utils import (\n",
    "    evaluate_classification, compare_models, plot_roc_curve,\n",
    "    plot_precision_recall_curve, plot_confusion_matrix,\n",
    "    plot_feature_importance, evaluate_recommender, create_evaluation_report\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464f61a",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from EDA notebook\n",
    "recipes = pd.read_csv('../datasets/recipes_clean.csv')\n",
    "interactions = pd.read_csv('../datasets/interactions_clean.csv')\n",
    "\n",
    "# Convert dates\n",
    "interactions['date'] = pd.to_datetime(interactions['date'])\n",
    "recipes['submitted'] = pd.to_datetime(recipes['submitted'])\n",
    "\n",
    "print(f\"Loaded {len(recipes)} recipes\")\n",
    "print(f\"Loaded {len(interactions)} interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bb512",
   "metadata": {},
   "source": [
    "## 2. Define Binary Target and Filter Sparse Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary 'like' target (rating >= 4)\n",
    "interactions = create_binary_target(interactions, rating_threshold=4)\n",
    "\n",
    "print(\"Binary target 'is_like' created:\")\n",
    "print(interactions['is_like'].value_counts())\n",
    "print(f\"\\nLike rate: {interactions['is_like'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sparse users and recipes\n",
    "print(\"Before filtering:\")\n",
    "print(f\"  Users: {interactions['user_id'].nunique()}\")\n",
    "print(f\"  Recipes: {interactions['recipe_id'].nunique()}\")\n",
    "print(f\"  Interactions: {len(interactions)}\")\n",
    "\n",
    "interactions_filtered = filter_sparse_users_recipes(\n",
    "    interactions, \n",
    "    min_user_interactions=5,\n",
    "    min_recipe_interactions=5\n",
    ")\n",
    "\n",
    "print(\"\\nAfter filtering:\")\n",
    "print(f\"  Users: {interactions_filtered['user_id'].nunique()}\")\n",
    "print(f\"  Recipes: {interactions_filtered['recipe_id'].nunique()}\")\n",
    "print(f\"  Interactions: {len(interactions_filtered)}\")\n",
    "print(f\"  Retained: {100 * len(interactions_filtered) / len(interactions):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512d4af",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user-level features\n",
    "user_features = compute_user_features(interactions_filtered, recipes)\n",
    "print(f\"Computed features for {len(user_features)} users\")\n",
    "display(user_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recipe-level features\n",
    "recipe_features = compute_recipe_features(interactions_filtered)\n",
    "print(f\"Computed features for {len(recipe_features)} recipes\")\n",
    "display(recipe_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full modeling dataset\n",
    "model_df = create_modeling_dataset(\n",
    "    interactions_filtered, \n",
    "    recipes, \n",
    "    user_features, \n",
    "    recipe_features\n",
    ")\n",
    "\n",
    "print(f\"Modeling dataset created: {len(model_df)} rows\")\n",
    "print(f\"\\nColumns: {list(model_df.columns)}\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "model_df_clean = model_df.dropna(subset=get_feature_columns() + ['is_like'])\n",
    "print(f\"\\nAfter dropping NaN: {len(model_df_clean)} rows ({100 * len(model_df_clean)/len(model_df):.1f}% retained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f84467",
   "metadata": {},
   "source": [
    "## 4. Train/Validation/Test Split (Temporal per User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal split per user\n",
    "train_df, val_df, test_df = split_temporal_per_user(\n",
    "    model_df_clean,\n",
    "    train_ratio=0.6,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.2\n",
    ")\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Train: {len(train_df)} rows, {train_df['user_id'].nunique()} users, {train_df['recipe_id'].nunique()} recipes\")\n",
    "print(f\"  Val:   {len(val_df)} rows, {val_df['user_id'].nunique()} users, {val_df['recipe_id'].nunique()} recipes\")\n",
    "print(f\"  Test:  {len(test_df)} rows, {test_df['user_id'].nunique()} users, {test_df['recipe_id'].nunique()} recipes\")\n",
    "\n",
    "print(\"\\nLike rates:\")\n",
    "print(f\"  Train: {train_df['is_like'].mean():.3f}\")\n",
    "print(f\"  Val:   {val_df['is_like'].mean():.3f}\")\n",
    "print(f\"  Test:  {test_df['is_like'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac427b",
   "metadata": {},
   "source": [
    "## 5. Prepare Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns\n",
    "feature_cols = get_feature_columns(for_modeling=True)\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, available_features = prepare_features_for_training(\n",
    "    train_df, val_df, test_df, feature_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nAvailable features ({len(available_features)}): {available_features}\")\n",
    "print(f\"\\nTrain shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val shape:   X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test shape:  X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d6a4a",
   "metadata": {},
   "source": [
    "## 6. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1: Global Average\n",
    "baseline_global = GlobalAverageBaseline()\n",
    "baseline_global.fit(y_train)\n",
    "\n",
    "y_test_proba_global = baseline_global.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_global = baseline_global.predict(X_test)\n",
    "\n",
    "results_global = evaluate_classification(\n",
    "    y_test, y_test_pred_global, y_test_proba_global, \"Global Average\"\n",
    ")\n",
    "\n",
    "print(\"Global Average Baseline:\")\n",
    "print(baseline_global)\n",
    "print(f\"  Test AUC: {results_global['roc_auc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {results_global['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 2: Recipe Average\n",
    "baseline_recipe = RecipeAverageBaseline()\n",
    "baseline_recipe.fit(train_df)\n",
    "\n",
    "y_test_proba_recipe = baseline_recipe.predict_proba(test_df)[:, 1]\n",
    "y_test_pred_recipe = (y_test_proba_recipe >= 0.5).astype(int)\n",
    "\n",
    "results_recipe = evaluate_classification(\n",
    "    y_test, y_test_pred_recipe, y_test_proba_recipe, \"Recipe Average\"\n",
    ")\n",
    "\n",
    "print(\"Recipe Average Baseline:\")\n",
    "print(baseline_recipe)\n",
    "print(f\"  Test AUC: {results_recipe['roc_auc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {results_recipe['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 3: User Average\n",
    "baseline_user = UserAverageBaseline()\n",
    "baseline_user.fit(train_df)\n",
    "\n",
    "y_test_proba_user = baseline_user.predict_proba(test_df)[:, 1]\n",
    "y_test_pred_user = (y_test_proba_user >= 0.5).astype(int)\n",
    "\n",
    "results_user = evaluate_classification(\n",
    "    y_test, y_test_pred_user, y_test_proba_user, \"User Average\"\n",
    ")\n",
    "\n",
    "print(\"User Average Baseline:\")\n",
    "print(baseline_user)\n",
    "print(f\"  Test AUC: {results_user['roc_auc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {results_user['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b78db",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters on validation set\n",
    "best_model, tuning_results = tune_hyperparameters(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    C_values=[0.01, 0.1, 1, 10, 100],\n",
    "    metric='roc_auc',\n",
    "    feature_names=available_features\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter tuning results:\")\n",
    "display(tuning_results)\n",
    "print(f\"\\nBest model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751898ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on all splits\n",
    "# Training\n",
    "y_train_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "results_train = evaluate_classification(y_train, y_train_pred, y_train_proba, \"Logistic Regression\")\n",
    "\n",
    "# Validation\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "results_val = evaluate_classification(y_val, y_val_pred, y_val_proba, \"Logistic Regression\")\n",
    "\n",
    "# Test\n",
    "y_test_proba_lr = best_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_lr = best_model.predict(X_test)\n",
    "results_test = evaluate_classification(y_test, y_test_pred_lr, y_test_proba_lr, \"Logistic Regression\")\n",
    "\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(\"=\"*60)\n",
    "report = create_evaluation_report(results_train, results_val, results_test)\n",
    "display(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3c7fa",
   "metadata": {},
   "source": [
    "## 8. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed47ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models on test set\n",
    "all_results = [\n",
    "    results_global,\n",
    "    results_recipe,\n",
    "    results_user,\n",
    "    results_test\n",
    "]\n",
    "\n",
    "comparison = compare_models(all_results, metric='roc_auc')\n",
    "\n",
    "print(\"Model Comparison (Test Set):\")\n",
    "print(\"=\"*80)\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20361639",
   "metadata": {},
   "source": [
    "## 9. Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "plot_roc_curve(y_test, y_test_proba_global, \"Global Average\", ax)\n",
    "plot_roc_curve(y_test, y_test_proba_recipe, \"Recipe Average\", ax)\n",
    "plot_roc_curve(y_test, y_test_proba_user, \"User Average\", ax)\n",
    "plot_roc_curve(y_test, y_test_proba_lr, \"Logistic Regression\", ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/roc_curve_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "plot_precision_recall_curve(y_test, y_test_proba_lr, \"Logistic Regression\", ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb23212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_confusion_matrix(y_test, y_test_pred_lr, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "coefficients_df = best_model.get_coefficients()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "display(coefficients_df.head(10))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_feature_importance(coefficients_df, top_n=20, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226d95a",
   "metadata": {},
   "source": [
    "## 10. Build Recipe Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df791831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recommender\n",
    "recommender = RecipeRecommender(best_model, available_features)\n",
    "print(f\"Recommender created with {len(available_features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate set for test users\n",
    "# For each user, we'll consider recipes they haven't rated in training\n",
    "test_users = test_df['user_id'].unique()[:100]  # Sample 100 users for efficiency\n",
    "\n",
    "print(f\"Creating candidates for {len(test_users)} test users...\")\n",
    "\n",
    "# Get recipes each user has already rated in train/val\n",
    "train_val_df = pd.concat([train_df, val_df])\n",
    "user_rated_recipes = train_val_df.groupby('user_id')['recipe_id'].apply(set).to_dict()\n",
    "\n",
    "# For each test user, create candidates from popular recipes they haven't rated\n",
    "popular_recipes = recipe_features.nlargest(500, 'recipe_num_ratings')['recipe_id'].values\n",
    "\n",
    "candidate_rows = []\n",
    "for user_id in test_users:\n",
    "    rated = user_rated_recipes.get(user_id, set())\n",
    "    candidates = [r for r in popular_recipes if r not in rated]\n",
    "    \n",
    "    for recipe_id in candidates[:50]:  # Top 50 candidates per user\n",
    "        candidate_rows.append({'user_id': user_id, 'recipe_id': recipe_id})\n",
    "\n",
    "candidates_df = pd.DataFrame(candidate_rows)\n",
    "print(f\"Created {len(candidates_df)} candidate pairs\")\n",
    "\n",
    "# Add features to candidates\n",
    "candidates_full = create_modeling_dataset(candidates_df, recipes, user_features, recipe_features)\n",
    "candidates_full = candidates_full.dropna(subset=available_features)\n",
    "\n",
    "print(f\"Candidates with features: {len(candidates_full)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92398f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations (without health adjustment)\n",
    "recommendations = recommender.recommend_batch(\n",
    "    test_users, \n",
    "    candidates_full, \n",
    "    top_k=10,\n",
    "    health_weight=0.0\n",
    ")\n",
    "\n",
    "print(f\"Generated recommendations for {len(recommendations)} users\")\n",
    "\n",
    "# Show example\n",
    "example_user = test_users[0]\n",
    "print(f\"\\nExample recommendations for user {example_user}:\")\n",
    "if example_user in recommendations:\n",
    "    display(recommendations[example_user])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b704f",
   "metadata": {},
   "source": [
    "## 11. Evaluate Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate recommender using Precision@K and Recall@K\n",
    "rec_metrics = evaluate_recommender(\n",
    "    test_df, \n",
    "    recommendations, \n",
    "    k_values=[5, 10]\n",
    ")\n",
    "\n",
    "print(\"Recommender Evaluation Metrics:\")\n",
    "print(\"=\"*40)\n",
    "for metric, value in rec_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5d2a8",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9953ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model evaluation results\n",
    "report.to_csv('../reports/model_evaluation.csv', index=False)\n",
    "comparison.to_csv('../reports/model_comparison.csv', index=False)\n",
    "\n",
    "# Save feature importance\n",
    "coefficients_df.to_csv('../reports/feature_importance.csv', index=False)\n",
    "\n",
    "# Save recommender metrics\n",
    "pd.DataFrame([rec_metrics]).to_csv('../reports/recommender_metrics.csv', index=False)\n",
    "\n",
    "print(\"Results saved to reports/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83d2f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Baseline Performance**: All baselines provide reasonable predictions, with recipe and user averages outperforming global average\n",
    "\n",
    "2. **Logistic Regression**: Achieves best performance by combining user, recipe, and nutrition features\n",
    "\n",
    "3. **Important Features**: Top predictors include user/recipe mean ratings, nutrition metrics, and healthiness indicators\n",
    "\n",
    "4. **Recommender System**: Successfully generates personalized recommendations with measurable precision and recall\n",
    "\n",
    "**Next Steps:**\n",
    "- Analyze health bias in recommendations\n",
    "- Explore trade-offs between recommendation quality and healthiness"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
